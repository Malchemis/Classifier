{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# soundNet: Classifying Vehicle Classes from Audio Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries/Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from statistics import mean\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torchaudio \n",
    "import torchvision \n",
    "from torchaudio.transforms import AmplitudeToDB, MelSpectrogram, Resample\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils import tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration cell\n",
    "# ------------------\n",
    "# This cell contains all the configuration for the notebook.\n",
    "\n",
    "# Path to the directory containing the sound files.\n",
    "data_dir = '/media/huimin/PortableSSD/KTH/data/SED-single_vehicle-30_sec/'\n",
    "annotations_path= '/media/huimin/PortableSSD/KTH/data/SED-single_vehicle-30_sec/weak_labels.csv'\n",
    "\n",
    "# Information on the dataset\n",
    "classes = [1, 2, 5, 12, 15]\n",
    "header = ['filepath_wav_30', 'vehicle_class', 'vehicle_class_combo', 'vehicle_class_base',\t'speed']\n",
    "_SEED = 42\n",
    "\n",
    "# Parameters for the spectrogram computation.\n",
    "feats = {\n",
    "    'n_mels': 128,\n",
    "    'n_filters': 2048,\n",
    "    'hop_length': 256,\n",
    "    'n_window': 2048,\n",
    "    'sample_rate': 16000,\n",
    "    'f_min': 0,\n",
    "    'f_max': 8000,\n",
    "    'duration': 30\n",
    "}\n",
    "\n",
    "# Training and model parameters\n",
    "model = {\n",
    "    'batch_size' : 12,\n",
    "    'lr' : 0.001,\n",
    "    'epochs' : 100,\n",
    "    'num_workers' : 4,\n",
    "    'n_frames' : 1876\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepath_wav_30</th>\n",
       "      <th>vehicle_class</th>\n",
       "      <th>vehicle_class_combo</th>\n",
       "      <th>vehicle_class_base</th>\n",
       "      <th>speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wav/20230109/rs_an0005_dt_20230109_tm_015511_t...</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>M</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wav/20230109/rs_an0005_dt_20230109_tm_022315_t...</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>M</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wav/20230109/rs_an0005_dt_20230109_tm_022947_t...</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>M</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wav/20230109/rs_an0005_dt_20230109_tm_023948_t...</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>M</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wav/20230109/rs_an0005_dt_20230109_tm_024134_t...</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>M</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     filepath_wav_30  vehicle_class  \\\n",
       "0  wav/20230109/rs_an0005_dt_20230109_tm_015511_t...              2   \n",
       "1  wav/20230109/rs_an0005_dt_20230109_tm_022315_t...              2   \n",
       "2  wav/20230109/rs_an0005_dt_20230109_tm_022947_t...              2   \n",
       "3  wav/20230109/rs_an0005_dt_20230109_tm_023948_t...              5   \n",
       "4  wav/20230109/rs_an0005_dt_20230109_tm_024134_t...              5   \n",
       "\n",
       "   vehicle_class_combo vehicle_class_base  speed  \n",
       "0                   16                  M     33  \n",
       "1                   16                  M     77  \n",
       "2                   16                  M     28  \n",
       "3                   18                  M     20  \n",
       "4                   18                  M     38  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the annotations in a pandas dataframe\n",
    "labels = pd.read_csv(annotations_path, sep='\\t')\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3303 unique filenames in the dataset.\n"
     ]
    }
   ],
   "source": [
    "# Compare the number of unique filenames and raw number of rows\n",
    "# to see if there are any duplicates.\n",
    "unique = labels[header[0]].nunique()\n",
    "length = labels.shape[0]\n",
    "if not np.array_equal(unique, length):\n",
    "    print(\"There are duplicates in the dataset.\")\n",
    "else:\n",
    "    print(f\"There are {length} unique filenames in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the classes are the correct ones\n",
    "unique_classes = labels[header[1]].unique()\n",
    "unique_classes.sort()\n",
    "if not np.array_equal(classes, unique_classes):\n",
    "    print('The classes are not the same as the ones in the annotations file.')\n",
    "    print('Classes in annotations file: {}'.format(unique_classes))\n",
    "    print('Classes in classes variable: {}'.format(classes))\n",
    "    raise ValueError('The classes are not the same as the ones in the annotations file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepath_wav_30</th>\n",
       "      <th>vehicle_class</th>\n",
       "      <th>vehicle_class_combo</th>\n",
       "      <th>vehicle_class_base</th>\n",
       "      <th>speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wav/20230109/rs_an0005_dt_20230109_tm_015511_t...</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>M</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wav/20230109/rs_an0005_dt_20230109_tm_022315_t...</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>M</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wav/20230109/rs_an0005_dt_20230109_tm_022947_t...</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>M</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wav/20230109/rs_an0005_dt_20230109_tm_023948_t...</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>M</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wav/20230109/rs_an0005_dt_20230109_tm_024134_t...</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>M</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     filepath_wav_30  vehicle_class  \\\n",
       "0  wav/20230109/rs_an0005_dt_20230109_tm_015511_t...              1   \n",
       "1  wav/20230109/rs_an0005_dt_20230109_tm_022315_t...              1   \n",
       "2  wav/20230109/rs_an0005_dt_20230109_tm_022947_t...              1   \n",
       "3  wav/20230109/rs_an0005_dt_20230109_tm_023948_t...              2   \n",
       "4  wav/20230109/rs_an0005_dt_20230109_tm_024134_t...              2   \n",
       "\n",
       "   vehicle_class_combo vehicle_class_base  speed  \n",
       "0                   16                  M     33  \n",
       "1                   16                  M     77  \n",
       "2                   16                  M     28  \n",
       "3                   18                  M     20  \n",
       "4                   18                  M     38  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert classes to be in range [0, num_classes - 1]\n",
    "# This is necessary for the cross-entropy loss.\n",
    "labels[header[1]] = labels[header[1]].apply(lambda x: classes.index(x))\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3 splits: train, validation and test\n",
    "# -------------------------------------------\n",
    "# We will create 3 splits: train, validation and test. The train split will be used to train the model, the validation\n",
    "# split will be used to evaluate the model during training and the test split will be used to evaluate the model after\n",
    "# training. We will use a 70/10/20 split.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into train and test\n",
    "train, test = train_test_split(labels, test_size=0.2, random_state=_SEED)\n",
    "# Split the train dataset into train and validation\n",
    "train, val = train_test_split(train, test_size=0.1, random_state=_SEED)\n",
    "\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "val.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique audio files: 3303 = 2377 + 265 + 661\n",
      "Number of train files: 2377\n",
      "Number of validation files: 265\n",
      "Number of test files: 661\n"
     ]
    }
   ],
   "source": [
    "# Verify the splits\n",
    "print(f\"Number of unique audio files: {length} = {len(train)} + {len(val)} + {len(test)}\") if len(labels) == len(train) + len(val) + len(test) else print('The splits are not correct.')\n",
    "print('Number of train files: {}'.format(len(train)))\n",
    "print('Number of validation files: {}'.format(len(val)))\n",
    "print('Number of test files: {}'.format(len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the partitions\n",
    "# ---------------------\n",
    "# The partitions will be together, identified by 3 keys (train, val, test) and the values will be the couple (filename,\n",
    "# class).\n",
    "if not os.path.exists(data_dir + 'partitions.pkl'):\n",
    "    # Generate the partitions\n",
    "    # ---------------------\n",
    "    partitions = {'train': [], 'val': [], 'test': []}\n",
    "\n",
    "    partitions['train'] = [(filename, class_value) for filename, class_value in zip(train[header[0]], train[header[1]])]\n",
    "    partitions['val'] = [(filename, class_value) for filename, class_value in zip(val[header[0]], val[header[1]])]\n",
    "    partitions['test'] = [(filename, class_value) for filename, class_value in zip(test[header[0]], test[header[1]])]\n",
    "    \n",
    "    # Save the partitions\n",
    "    # -------------------\n",
    "    with open(data_dir + 'partitions.pkl', 'wb') as f:\n",
    "        pickle.dump(partitions, f)\n",
    "else:\n",
    "    with open(data_dir + 'partitions.pkl', 'rb') as f:\n",
    "        partitions = pickle.load(f)\n",
    "# Verify the partitions\n",
    "if len(partitions['train']) + len(partitions['val']) + len(partitions['test']) != len(labels):\n",
    "    print('The partitions are not correct.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing of the audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_audio(audio, target_len, fs):\n",
    "    \n",
    "    if audio.shape[-1] < target_len:\n",
    "        audio = torch.nn.functional.pad(\n",
    "            audio, (0, target_len - audio.shape[-1]), mode=\"constant\"\n",
    "        )\n",
    "\n",
    "        padded_indx = [target_len / len(audio)]\n",
    "        onset_s = 0.000\n",
    "    \n",
    "    elif len(audio) > target_len:\n",
    "        \n",
    "        rand_onset = random.randint(0, len(audio) - target_len)\n",
    "        audio = audio[rand_onset:rand_onset + target_len]\n",
    "        onset_s = round(rand_onset / fs, 3)\n",
    "        padded_indx = [target_len / len(audio)] \n",
    "\n",
    "    else:\n",
    "        onset_s = 0.000\n",
    "        padded_indx = [1.0]\n",
    "\n",
    "    offset_s = round(onset_s + (target_len / fs), 3)\n",
    "    return audio, onset_s, offset_s, padded_indx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_melspectrogram(audio, sample_rate, window_size, hop_size, n_mels, f_min, f_max):\n",
    "    \"\"\"Compute log melspectrogram of an audio signal.\"\"\"\n",
    "    # Compute the mel spectrogram\n",
    "    mel_spectrogram = MelSpectrogram(\n",
    "        sample_rate=sample_rate,\n",
    "        n_fft=window_size,\n",
    "        win_length=window_size,\n",
    "        hop_length=hop_size,\n",
    "        f_min=f_min,\n",
    "        f_max=f_max,\n",
    "        n_mels=n_mels,\n",
    "        window_fn=torch.hamming_window,\n",
    "        wkwargs={\"periodic\": False},\n",
    "        power=1,\n",
    "    )(audio)\n",
    "    # Convert to dB\n",
    "    amp_to_db = AmplitudeToDB(stype=\"amplitude\")\n",
    "    amp_to_db.amin = 1e-5  # amin= 1e-5 as in librosa\n",
    "    log_melspectrogram = amp_to_db(mel_spectrogram).clamp(min=-50, max=80)\n",
    "    return log_melspectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_melspectrogram_set(set, save_path, sample_rate=feats[\"sample_rate\"], window_size=feats[\"n_window\"], hop_size=feats[\"hop_length\"], n_mels=feats[\"n_mels\"], f_min=feats[\"f_min\"], f_max=feats[\"f_max\"], duration=feats[\"duration\"]): \n",
    "    \"\"\"Compute log melspectrogram of a set of audio signals.\"\"\"\n",
    "    for i, filename in enumerate(set[header[0]].unique()):\n",
    "        print(f\"\\rConstructing mel audio {i+1}/{len(set[header[0]].unique())}\", flush=True)\n",
    "        audio, sr = torchaudio.load(os.path.join(data_dir + filename))\n",
    "        if sr != sample_rate:\n",
    "            resampled_audio = Resample(sr, sample_rate)(audio)\n",
    "        resampled_audio_pad, *_ = pad_audio(resampled_audio, duration*sample_rate, sample_rate)\n",
    "        log_melspectrogram = get_log_melspectrogram(resampled_audio_pad, sample_rate, window_size, hop_size, n_mels, f_min, f_max)\n",
    "        # Create the save path folder if doesn't exist\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "        np.save(f\"{save_path}/{filename.split('/')[-1].replace('.wav', '')}.npy\", log_melspectrogram.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the log melspectrogram for each set\n",
    "# -------------------------------------------\n",
    "if not os.path.exists(os.path.join(data_dir + 'npy', 'train')):\n",
    "    print(\"Constructing mel audio for train set\")\n",
    "    get_log_melspectrogram_set(train, os.path.join(data_dir + 'npy', 'train'))\n",
    "if not os.path.exists(os.path.join(data_dir + 'npy', 'val')):    \n",
    "    print(\"Constructing mel audio for val set\")\n",
    "    get_log_melspectrogram_set(val, os.path.join(data_dir + 'npy', 'val'))\n",
    "if not os.path.exists(os.path.join(data_dir + 'npy', 'test')):\n",
    "    print(\"Constructing mel audio for test set\")\n",
    "    get_log_melspectrogram_set(test, os.path.join(data_dir + 'npy', 'test'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset and the dataloaders\n",
    "class VehicleDataset(Dataset): \n",
    "    def __init__(self, data_dir, partition, set='train', n_frames=model['n_frames']):\n",
    "        self.partition = partition[set]\n",
    "        self.data_dir = data_dir\n",
    "        self.set = set\n",
    "        self.n_frames = n_frames\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.partition)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        filename, class_value = self.partition[idx]\n",
    "        log_melspectrogram = np.load(os.path.join(self.data_dir + 'npy', self.set, filename.split('/')[-1].replace('.wav', '') + '.npy'))[:, :, :self.n_frames]\n",
    "        return log_melspectrogram, class_value\n",
    "    \n",
    "train_dataloader = DataLoader(VehicleDataset(data_dir, partitions), batch_size=model['batch_size'], shuffle=True, num_workers=model['num_workers'])\n",
    "val_dataloader = DataLoader(VehicleDataset(data_dir, partitions, set='val'), batch_size=model['batch_size'], shuffle=True, num_workers=model['num_workers'])\n",
    "test_dataloader = DataLoader(VehicleDataset(data_dir, partitions, set='test'), batch_size=model['batch_size'], shuffle=True, num_workers=model['num_workers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=5, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load VGG16 model \n",
    "vgg16 = torchvision.models.get_model('vgg16', weights=None)\n",
    "# Change the input layer\n",
    "vgg16.features[0] = torch.nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "# Modify the last layer\n",
    "vgg16.classifier[6] = torch.nn.Linear(4096, len(classes))\n",
    "print(vgg16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loader, writer, epochs=10):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = []\n",
    "        t = tqdm(loader)\n",
    "        for x, y in t:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            running_loss.append(loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            t.set_description(f'training loss: {mean(running_loss)}')\n",
    "        writer.add_scalar('training loss', mean(running_loss), epochs)\n",
    "\n",
    "def test(model, dataloader):\n",
    "    test_corrects = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = model(x).argmax(1)\n",
    "            test_corrects += y_hat.eq(y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return test_corrects / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/199 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 416.9128973881404:   3%|▎         | 6/199 [02:04<1:06:42, 20.74s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/huimin/Documents/INSA/ModIA/PFE_KTH/Code/Classifier/deepC/soundNet.ipynb Cell 26\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/huimin/Documents/INSA/ModIA/PFE_KTH/Code/Classifier/deepC/soundNet.ipynb#Y115sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(vgg16\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mmodel[\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/huimin/Documents/INSA/ModIA/PFE_KTH/Code/Classifier/deepC/soundNet.ipynb#Y115sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m writer \u001b[39m=\u001b[39m tensorboard\u001b[39m.\u001b[39mSummaryWriter()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/huimin/Documents/INSA/ModIA/PFE_KTH/Code/Classifier/deepC/soundNet.ipynb#Y115sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m train(vgg16, optimizer, train_dataloader, writer, epochs\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)\n",
      "\u001b[1;32m/home/huimin/Documents/INSA/ModIA/PFE_KTH/Code/Classifier/deepC/soundNet.ipynb Cell 26\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/huimin/Documents/INSA/ModIA/PFE_KTH/Code/Classifier/deepC/soundNet.ipynb#Y115sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m x, y \u001b[39min\u001b[39;00m t:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/huimin/Documents/INSA/ModIA/PFE_KTH/Code/Classifier/deepC/soundNet.ipynb#Y115sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     x, y \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(device), y\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/huimin/Documents/INSA/ModIA/PFE_KTH/Code/Classifier/deepC/soundNet.ipynb#Y115sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model(x)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/huimin/Documents/INSA/ModIA/PFE_KTH/Code/Classifier/deepC/soundNet.ipynb#Y115sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(outputs, y)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/huimin/Documents/INSA/ModIA/PFE_KTH/Code/Classifier/deepC/soundNet.ipynb#Y115sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     running_loss\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/anaconda3/envs/dcase2023/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/dcase2023/lib/python3.8/site-packages/torchvision/models/vgg.py:66\u001b[0m, in \u001b[0;36mVGG.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m---> 66\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures(x)\n\u001b[1;32m     67\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavgpool(x)\n\u001b[1;32m     68\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(x, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/dcase2023/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/dcase2023/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/dcase2023/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/dcase2023/lib/python3.8/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/anaconda3/envs/dcase2023/lib/python3.8/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vgg16 = vgg16.to(device)\n",
    "optimizer = torch.optim.Adam(vgg16.parameters(), lr=model['lr'])\n",
    "writer = tensorboard.SummaryWriter()\n",
    "train(vgg16, optimizer, train_dataloader, writer, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = test(vgg16, test_dataloader)\n",
    "print(f'Test accuracy:{test_acc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sNet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
